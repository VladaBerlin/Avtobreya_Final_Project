{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальный проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Притворяемся браузером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для сбора текстов для корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://rustih.ru/stixi-o-prirode/'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "corpora = {}\n",
    "\n",
    "for element in soup.find_all('div', {'class': 'entry-title'}):\n",
    "    title = element.text\n",
    "    link = element.find('a').attrs['href']\n",
    "    if link != 'https://rustih.ru/fedor-tyutchev-ne-to-chto-mnite-vy-priroda/': # почему-то спотыкается, а еще там _______ как разделитель используется неожиданно, поэтому это проигнорируем\n",
    "        req_poem = session.get(link, headers={'User-Agent': ua.random})\n",
    "        poem = req_poem.text\n",
    "        soup_poem = BeautifulSoup(poem, 'html.parser')\n",
    "        text = soup_poem.find('div', {'class': 'entry-content poem-text'}).text.replace('I', '')    # У некоторых стихотворений указываются номера четверостиший римскими цифрами, поэтому нам нужно избавиться от букв, которые используются при записи римским цифр\n",
    "        text = text.replace('X', '')\n",
    "        text = text.replace('V', '')\n",
    "        text = sent_tokenize(text.lstrip())\n",
    "        cleared_text = text.copy()\n",
    "        for sent in text:\n",
    "            if len(sent) < 3:\n",
    "                cleared_text.remove(sent)\n",
    "            elif '______' in sent:  #Эта штука используется как разделитель между стихотворением и его анализом, анализ нам не слишком нужен, поэтому можно убирать\n",
    "                cleared_text = cleared_text[:cleared_text.index(sent)]\n",
    "        corpora[title] = cleared_text\n",
    "        if len(corpora) == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для сбора всех текстов в корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_corpus.csv', 'a', encoding='utf8')as f:\n",
    "    fieldnames = ['id_sent', 'id_word', 'word', 'POS', 'lemma', 'sentence']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for k,v in corpora.items():\n",
    "        for st in v:\n",
    "            doc = Doc(st)\n",
    "            sent = str(st) + ' [' + str(k) + ']\\n'\n",
    "            doc.segment(segmenter)\n",
    "            doc.tag_morph(morph_tagger)\n",
    "            for token in doc.tokens:\n",
    "                token.lemmatize(morph_vocab)\n",
    "            j = 0\n",
    "            for el in doc.tokens:\n",
    "                writer.writerow({'id_sent': i, 'id_word': j, 'word': el.text.lower(),\n",
    "                'POS': el.pos, 'lemma': el.lemma, 'sentence': sent})\n",
    "                j += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для поиска по корпусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая осуществляем поиск по корпусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(query):\n",
    "    query = query.split()\n",
    "    result = []\n",
    "    id = ''\n",
    "    word = ''\n",
    "    s = ''\n",
    "\n",
    "    with open('my_corpus.csv', encoding='utf8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        if len(query) == 1: # Если запрос состоит из одного слова\n",
    "            if query[0] not in tags and query[0][0] != '\"':\n",
    "                q = query[0]\n",
    "                if '+' in query[0]:\n",
    "                    q = query[0].split('+')[0]\n",
    "                doc = Doc(q)\n",
    "                doc.segment(segmenter)\n",
    "                doc.tag_morph(morph_tagger)\n",
    "                for token in doc.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "\n",
    "            for row in reader:\n",
    "                if id:\n",
    "                    if row['id_sent'] == id:    # Если это предложение уже записано в выдачу, то мы игнорируем все оставшиеся слова из него, потому что нет смысла несколько раз выводить одно и то же предложение\n",
    "                        continue\n",
    "                if query[0] in tags:\n",
    "                    if row['POS'] == query[0]:\n",
    "                        result.append(row['sentence'])\n",
    "                        id = row['id_sent']\n",
    "                elif query[0][0] == '\"':\n",
    "                    if row['word'] == query[0][1:len(query[0])-1]:\n",
    "                        result.append(row['sentence'])\n",
    "                        id = row['id_sent']\n",
    "                elif '+' in query[0]:\n",
    "                    w, p = query[0].split('+')\n",
    "                    if doc.tokens[0].lemma == row['lemma'] and p == row['POS']:\n",
    "                        result.append(row['sentence'])\n",
    "                        id = row['id_sent']\n",
    "                else:\n",
    "                    if row['lemma'] == doc.tokens[0].lemma or row['lemma'] == query[0]:\n",
    "                        result.append(row['sentence'])\n",
    "                        id = row['id_sent']\n",
    "\n",
    "        elif len(query) == 2:   # Если запрос состоит из двух слов\n",
    "            if query[0] not in tags and query[0][0] != '\"':\n",
    "                q1 = query[0]\n",
    "                if '+' in query[0]:\n",
    "                    q1 = query[0].split('+')[0]\n",
    "                doc1 = Doc(q1)\n",
    "                doc1.segment(segmenter)\n",
    "                doc1.tag_morph(morph_tagger)\n",
    "                for token in doc1.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "\n",
    "            if query[1] not in tags and query[1][0] != '\"':\n",
    "                q2 = query[1]\n",
    "                if '+' in query[1]:\n",
    "                    q2 = query[1].split('+')[0]\n",
    "                doc2 = Doc(q2)\n",
    "                doc2.segment(segmenter)\n",
    "                doc2.tag_morph(morph_tagger)\n",
    "                for token in doc2.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "                    \n",
    "            for row in reader:\n",
    "                if id:\n",
    "                    if id == row['id_sent']:\n",
    "                        continue\n",
    "\n",
    "                if query[0] in tags:\n",
    "                    if row['POS'] == query[0]:\n",
    "                        word = row['id_word']\n",
    "                        s = row['id_sent']\n",
    "                elif query[0][0] == '\"':\n",
    "                    if row['word'] == query[0][1:len(query[0])-1]:\n",
    "                        word = row['id_word']\n",
    "                        s = row['id_sent']\n",
    "                elif '+' in query[0]:\n",
    "                        w, p = query[0].split('+')\n",
    "                        if doc1.tokens[0].lemma == row['lemma'] and p == row['POS']:\n",
    "                            word = row['id_word']\n",
    "                            s = row['id_sent']\n",
    "                else:\n",
    "                        if row['lemma'] == doc1.tokens[0].lemma or row['lemma'] == query[0]:\n",
    "                            word = row['id_word']\n",
    "                            s = row['id_sent']\n",
    "\n",
    "                if s != row['id_sent']:\n",
    "                    word = ''\n",
    "\n",
    "                if word:\n",
    "                    if query[1] in tags:\n",
    "                        if row['POS'] == query[1] and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "                    elif query[1][0] == '\"':\n",
    "                        if row['word'] == query[1][1:len(query[1])-1] and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "                    elif '+' in query[1]:\n",
    "                        w, p = query[1].split('+')\n",
    "                        if doc2.tokens[0].lemma == row['lemma'] and p == row['POS'] and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "                    else:\n",
    "                        if (row['lemma'] == doc2.tokens[0].lemma or row['lemma'] == query[0]) and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "\n",
    "        elif len(query) == 3:   # Если запрос состоит из трех слов\n",
    "            if query[0] not in tags and query[0][0] != '\"':\n",
    "                q1 = query[0]\n",
    "                if '+' in query[0]:\n",
    "                    q1 = query[0].split('+')[0]\n",
    "                doc1 = Doc(q1)\n",
    "                doc1.segment(segmenter)\n",
    "                doc1.tag_morph(morph_tagger)\n",
    "                for token in doc1.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "            if query[1] not in tags and query[1][0] != '\"':\n",
    "                q2 = query[1]\n",
    "                if '+' in query[1]:\n",
    "                    q2 = query[1].split('+')[0]\n",
    "                doc2 = Doc(q2)\n",
    "                doc2.segment(segmenter)\n",
    "                doc2.tag_morph(morph_tagger)\n",
    "                for token in doc2.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "            if query[2] not in tags and query[2][0] != '\"':\n",
    "                q3 = query[2]\n",
    "                if '+' in query[2]:\n",
    "                    q3 = query[2].split('+')[0]\n",
    "                doc3 = Doc(q3)\n",
    "                doc3.segment(segmenter)\n",
    "                doc3.tag_morph(morph_tagger)\n",
    "                for token in doc3.tokens:\n",
    "                    token.lemmatize(morph_vocab)\n",
    "                    \n",
    "            for row in reader:\n",
    "                if id:\n",
    "                    if id == row['id_sent']:\n",
    "                        continue\n",
    "\n",
    "                if query[0] in tags:\n",
    "                    if row['POS'] == query[0]:\n",
    "                        word = row['id_word']\n",
    "                        s = row['id_sent']\n",
    "\n",
    "                elif query[0][0] == '\"':\n",
    "                    if row['word'] == query[0][1:len(query[0])-1]:\n",
    "                        word = row['id_word']\n",
    "                        s = row['id_sent']\n",
    "\n",
    "                elif '+' in query[0]:\n",
    "                        w, p = query[0].split('+')\n",
    "                        if doc1.tokens[0].lemma == row['lemma'] and p == row['POS']:\n",
    "                            word = row['id_word']\n",
    "                            s = row['id_sent']\n",
    "\n",
    "                else:\n",
    "                        if row['lemma'] == doc1.tokens[0].lemma or row['lemma'] == query[0]:\n",
    "                            word = row['id_word']\n",
    "                            s = row['id_sent']\n",
    "\n",
    "                if s != row['id_sent']:\n",
    "                    word = ''\n",
    "\n",
    "                if word:\n",
    "                    if query[1] in tags:\n",
    "                        if row['POS'] == query[1] and int(word) + 1 == int(row['id_word']):\n",
    "                            word = row['id_word']\n",
    "                    elif query[1][0] == '\"':\n",
    "                        if row['word'] == query[1][1:len(query[1])-1] and int(word) + 1 == int(row['id_word']):\n",
    "                            word = row['id_word']\n",
    "                    elif '+' in query[1]:\n",
    "                            w, p = query[1].split('+')\n",
    "                            if doc2.tokens[0].lemma == row['lemma'] and p == row['POS'] and int(word) + 1 == int(row['id_word']):\n",
    "                                word = row['id_word']\n",
    "                    else:\n",
    "                            if (row['lemma'] == doc2.tokens[0].lemma or row['lemma'] == query[1]) and int(word) + 1 == int(row['id_word']):\n",
    "                                word = row['id_word']\n",
    "\n",
    "                    if query[2] in tags:\n",
    "                        if row['POS'] == query[2] and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "                    elif query[2][0] == '\"':\n",
    "                        if row['word'] == query[2][1:len(query[2])-1] and int(word) + 1 == int(row['id_word']):\n",
    "                            result.append(row['sentence'])\n",
    "                            id = row['id_sent']\n",
    "                    elif '+' in query[2]:\n",
    "                            w, p = query[2].split('+')\n",
    "                            if doc3.tokens[0].lemma == row['lemma'] and p == row['POS'] and int(word) + 1 == int(row['id_word']):\n",
    "                                result.append(row['sentence'])\n",
    "                                id = row['id_sent']\n",
    "                    else:\n",
    "                            if (row['lemma'] == doc3.tokens[0].lemma or row['lemma'] == query[2]) and int(word) + 1 == int(row['id_word']):\n",
    "                                result.append(row['sentence'])\n",
    "                                id = row['id_sent']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь можно сделать запрос и посмотреть на выдачу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['На весь его недолгий роздых\\nМы целый дом ему сдаем. [\\xa0Борис Пастернак — Июль]\\n',\n",
       " 'Для Есенина, уже познакомившегося со столичной жизнью и успевшего на нее насмотреться, береза была также символом родного дома. [\\xa0Сергей Есенин — Белая береза под моим окном]\\n',\n",
       " 'Ветвь и пальма символизируют любящих людей, тоскующих в разлуке, а так же разобщенные части одной души, некую оторванность от родного дома (от самого себя). [\\xa0Михаил Лермонтов — Ветка Палестины]\\n',\n",
       " 'Теперь становится ясно, что защитить от жизненных невзгод могут только личные силы героя, настрой на позитивное, а не стены родного дома. [Александр Пушкин — Зимний вечер (Буря мглою небо кроет)]\\n']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched = input()\n",
    "searching(searched)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dc2e76d637f38cb534d007662ee080514e23b3d0d0dd6e2f651136baa4a9222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
